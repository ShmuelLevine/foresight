{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8fc6c3e84f282d0e6f664caf9848297b6c62a16"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import time, random\n",
    "from numba import jit\n",
    "print(\"done\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "from itertools import chain\n",
    "from skimage.io import imread, imshow, concatenate_images\n",
    "from skimage.transform import resize\n",
    "from skimage.morphology import label\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input,Dropout,BatchNormalization,Activation,Add,UpSampling2D\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img#,save_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set some parameters\n",
    "img_size_ori = 101\n",
    "img_size_target = 128\n",
    "im_width = 101\n",
    "im_height = 101\n",
    "im_chan = 1\n",
    "basicpath = '../input/'\n",
    "path_train = basicpath + 'train/'\n",
    "path_test = basicpath + 'test/'\n",
    "\n",
    "path_train_images = path_train + 'images/'\n",
    "path_train_masks = path_train + 'masks/'\n",
    "path_test_images = path_test + 'images/'\n",
    "\n",
    "def upsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n",
    "    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n",
    "    #res[:img_size_ori, :img_size_ori] = img\n",
    "    #return res\n",
    "    \n",
    "def downsample(img):\n",
    "    if img_size_ori == img_size_target:\n",
    "        return img\n",
    "    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n",
    "    #return img[:img_size_ori, :img_size_ori]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1a64babef03b9a0dbc94387a1dad54971c3e028d",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Loading of training/testing ids and depths\n",
    "\n",
    "train_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\n",
    "depths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\n",
    "train_df = train_df.join(depths_df)\n",
    "test_df = depths_df[~depths_df.index.isin(train_df.index)]\n",
    "print(len(depths_df))\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80c3768717007fb5f087d3e01619f1a9f9a3beac",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "def my_load_image(file):\n",
    "#img = cv2.imread('../input/train/images/9f3b8d0186.png')\n",
    "    img = cv2.imread(file)\n",
    "    img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n",
    "    img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n",
    "    hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\n",
    "    #median = cv2.medianBlur(hist_equalization_result,5)\n",
    "    #kernel = np.ones((5,5),np.float32)/25\n",
    "    #dst = cv2.filter2D(hist_equalization_result,-1,kernel)\n",
    "    return hist_equalization_result\n",
    "\n",
    "train_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f55103f7daad6f03ec874c643077fe686c31bee",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "010066dd50ef4fdfa7dabe2c946fd7491f9556fd",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n",
    "\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "        \n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2ad5ac1576277fc54d768933c36efd1f9ff01acd",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15,5))\n",
    "sns.distplot(train_df.coverage, kde=False, ax=axs[0])\n",
    "sns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\n",
    "plt.suptitle(\"Salt coverage\")\n",
    "axs[0].set_xlabel(\"Coverage\")\n",
    "axs[1].set_xlabel(\"Coverage class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f34c4263989a3d95af1e5922c1b7d2126655610",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Plotting the depth distributionsÂ¶\n",
    "\n",
    "sns.distplot(train_df.z, label=\"Train\")\n",
    "sns.distplot(test_df.z, label=\"Test\")\n",
    "plt.legend()\n",
    "plt.title(\"Depth distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51981795f0dd6b8ca7abe4db367f48313b63811e",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create train/validation split stratified by salt coverage\n",
    "ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n",
    "    train_df.index.values,\n",
    "    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n",
    "    train_df.coverage.values,\n",
    "    train_df.z.values,\n",
    "    test_size=0.1, stratify=train_df.coverage_class, random_state=1337)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b9d158f567c829c55139acc9e79a41761d911726",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "iou_thresholds = np.array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n",
    "\n",
    "def fitnes_lossfunction(imgs_true, imgs_pred):\n",
    "    num_images = len(imgs_true)\n",
    "    scores = np.zeros(num_images)\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        if imgs_true[i].sum() == imgs_pred[i].sum() == 0:\n",
    "            scores[i] = 1\n",
    "        else:\n",
    "            scores[i] = (iou_thresholds <= iou(imgs_true[i], imgs_pred[i])).mean()\n",
    "            \n",
    "    return scores.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "072ab621d38cc93d26998f391357cb6efc791600",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Data augmentation\n",
    "x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n",
    "y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\n",
    "print(x_train.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8253792cb5a6ad464d03073b282df45d2a975f40",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def small_ae():\n",
    "    input_img = Input(shape=(128, 128, 1))  # adapt this if using `channels_first` image data format\n",
    "    net = Conv2D( 32, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform')(input_img)\n",
    "    net = Conv2D(16, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform')(net)\n",
    "    net = Conv2D( 8, (5, 5), strides=4, padding='SAME',kernel_initializer='random_uniform')(net)\n",
    "    # decoder\n",
    "    # 2 x 2 x 8    ->  8 x 8 x 16\n",
    "    # 8 x 8 x 16   ->  16 x 16 x 32\n",
    "    # 16 x 16 x 32  ->  32 x 32 x 1\n",
    "    net = Conv2DTranspose( 16, (5, 5), strides=4, padding='SAME',kernel_initializer='random_uniform')(net)\n",
    "    net = Conv2DTranspose( 32, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform')(net)\n",
    "    decoded  = Conv2DTranspose( 1, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform', activation='sigmoid')(net)\n",
    "    \n",
    "\n",
    "    autoencoder = Model(input_img, decoded )\n",
    "    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy',metrics=['acc'])\n",
    "    return autoencoder\n",
    "\n",
    "def small_unet():\n",
    "    inputs = Input((128, 128, 1))\n",
    "\n",
    "\n",
    "    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)\n",
    "    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n",
    "    p1 = MaxPooling2D((2, 2)) (c1)\n",
    "\n",
    "    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n",
    "    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n",
    "    p2 = MaxPooling2D((2, 2)) (c2)\n",
    "\n",
    "    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n",
    "    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n",
    "    p3 = MaxPooling2D((2, 2)) (c3)\n",
    "\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n",
    "    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
    "\n",
    "    c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n",
    "    c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n",
    "\n",
    "    u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n",
    "    c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n",
    "\n",
    "    u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n",
    "    c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n",
    "\n",
    "    u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n",
    "    c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n",
    "\n",
    "    u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n",
    "    c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "small_unet().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e57e7902202404581c3c53c097a1db264e0f077",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def model_crossover(model1, model2):\n",
    "    model=small_ae()\n",
    "    #for each layer\n",
    "    w1=model1.get_weights()\n",
    "    w2=model2.get_weights()\n",
    "    #print(type(w1))    \n",
    "    #print(len(w1))\n",
    "    wf=[]\n",
    "    for x in range(len(w1)):\n",
    "        if len(w1[x].shape)==4:\n",
    "     #       print(\"4\")\n",
    "            xx=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2],w1[x].shape[3])    \n",
    "            mutation_choise=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2],w1[x].shape[3])    \n",
    "            mutation_val=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2],w1[x].shape[3])*2            \n",
    "            z = np.where(xx>0.5, w1[x], w2[x])\n",
    "            z = np.where(mutation_choise>0.85,mutation_val*z,z)\n",
    "            wf.append(z)\n",
    "        if len(w1[x].shape)==3:\n",
    "      #      print(\"3\")\n",
    "            xx=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2])    \n",
    "            mutation_choise=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2])    \n",
    "            mutation_val=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2])*2            \n",
    "            z = np.where(xx>0.5, w1[x], w2[x])\n",
    "            z = np.where(mutation_choise>0.85,mutation_val*z,z)\n",
    "            wf.append(z)\n",
    "        if len(w1[x].shape)==2:\n",
    "       #     print(\"2\")\n",
    "            xx=np.random.rand(w1[x].shape[0],w1[x].shape[1])    \n",
    "            mutation_choise=np.random.rand(w1[x].shape[0],w1[x].shape[1])    \n",
    "            mutation_val=np.random.rand(w1[x].shape[0],w1[x].shape[1])*2            \n",
    "            z = np.where(xx>0.5, w1[x], w2[x])\n",
    "            z = np.where(mutation_choise>0.85,mutation_val*z,z)\n",
    "            wf.append(z)\n",
    "        if len(w1[x].shape)==1:\n",
    "        #    print(\"1\")\n",
    "            xx=np.random.rand(w1[x].shape[0])    \n",
    "            mutation_choise=np.random.rand(w1[x].shape[0])    \n",
    "            mutation_val=np.random.rand(w1[x].shape[0])*2            \n",
    "            z = np.where(xx>0.5, w1[x], w2[x])\n",
    "            z = np.where(mutation_choise>0.85,mutation_val*z,z)      \n",
    "   #         z=np.asscalar(z)\n",
    "            wf.append(z)\n",
    "            #print(z.shape)\n",
    "                                        \n",
    "    model.set_weights(wf)\n",
    " \n",
    "    return model\n",
    "      \n",
    " \n",
    "def runtournament():\n",
    "    list_idx_on_tournament=[]\n",
    "    for x in range (tournament_sel):\n",
    "        list_idx_on_tournament.append(int(random.uniform(0, total_models-1)))\n",
    "        \n",
    "    best1=-999999999999999\n",
    "    best2=-999999999999999\n",
    "    best1_idx=-999999999999999\n",
    "    best2_idx=-999999999999999\n",
    "    for  x in range (tournament_sel):\n",
    "        if fitness[list_idx_on_tournament[x]]>best1:\n",
    "            best1=fitness[x]\n",
    "            best1_idx=x\n",
    "            \n",
    "    for  x in range (tournament_sel):\n",
    "        if fitness[list_idx_on_tournament[x]]>best2 and x!=best1_idx:\n",
    "            best2=fitness[x]\n",
    "            best2_idx=x        \n",
    "    return current_pool[list_idx_on_tournament[best1_idx]],current_pool[list_idx_on_tournament[best2_idx]]\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "def filter_image(img):\n",
    "    if img.sum() < 100:\n",
    "        return np.zeros(img.shape)\n",
    "    else:\n",
    "        return img\n",
    "\n",
    "## Scoring for last model\n",
    "#thresholds = np.linspace(0.3, 0.7, 31)\n",
    "\n",
    "\n",
    "def parallel_scoring(i,x_train,y_train):    \n",
    "    \n",
    "    preds_valid=current_pool[x].predict(x_train)        \n",
    "    ious = fitnes_lossfunction(y_train.reshape((-1, img_size_target, img_size_target)), [filter_image(img) for img in preds_valid > 0.5 ])    \n",
    "    \n",
    "    return ious\n",
    "    \n",
    "    \n",
    "def parallel_muttion(i):\n",
    "    model1,model2=runtournament()\n",
    "    model=model_crossover(model1,model2)\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e04865b70013b1199b3e39c4ceae1548327adfdd",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "current_pool = [] #actual networks saved\n",
    "fitness = [] #save value for each network\n",
    "total_models = 75 #we load 75 models in random state\n",
    "generations=50 #we perform and train GA over 50 generationand end\n",
    "bestfitness_index=0\n",
    "tournament_sel=5 #usefull for tournament selection method\n",
    "        \n",
    "    \n",
    "# Initialize all models with random weigth\n",
    "for i in range(total_models):\n",
    "    # model\n",
    "    model=small_ae()\n",
    "    weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    model.set_weights(weights)\n",
    "    #model.summary()\n",
    "    current_pool.append(model)\n",
    "    fitness.append(-100)    \n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48c1da3414a909a8dd6dd03ca653d03ee18502ac",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"start generations\")\n",
    "\n",
    "\n",
    "#num_cores = multiprocessing.cpu_count()\n",
    "#inputs = range(total_models)\n",
    "\n",
    "for i in range(generations):\n",
    "        print(\"Generation: \",i)\n",
    "        results=[]\n",
    "        #with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            #for out1 in executor.map(parallel_scoring, range(0, total_models)):\n",
    "                #results.append(out1)\n",
    "        #xx_train,yy_train=next_batch(128,x_train,y_train)\n",
    "        for x in range(total_models):\n",
    "            results.append(parallel_scoring(x,x_train,y_train))                \n",
    "#        results = Parallel(n_jobs=num_cores)(delayed(parallel_scoring)(current_pool[i]) for i in inputs)\n",
    "        \n",
    "        \n",
    "        fitness=results    \n",
    "        bestfitness_index = np.argmax(np.array(fitness)) \n",
    "        print ('BEst Fitnes val {}- best fitnes index {}',fitness[bestfitness_index],bestfitness_index)\n",
    "        best_model=current_pool[bestfitness_index]#we save this model and save this as last\n",
    "        new_pool=[]        \n",
    "        for x in range(total_models-1):\n",
    "            model=parallel_muttion(i)\n",
    "            new_pool.append(model)        \n",
    "            #with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "            #for out1 in executor.map(parallel_muttion, range(0, total_models-1)):\n",
    "             #   new_pool.append(out1)                                    \n",
    "        current_pool=new_pool\n",
    "        current_pool.append(best_model)\n",
    "        \n",
    "            \n",
    "\n",
    "print ('BEst Fitnes val {}- best fitnes index {}',fitness[bestfitness_index],bestfitness_index)\n",
    "            \n",
    "            \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
